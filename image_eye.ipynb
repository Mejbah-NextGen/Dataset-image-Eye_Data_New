{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5f3307ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.optimizers import Adam, Adamax\n",
    "from tensorflow.keras.metrics import categorical_crossentropy\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import imshow\n",
    "import seaborn as sns\n",
    "sns.set_style('darkgrid')\n",
    "from sklearn.metrics import confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ffcd0971",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of files:  16016    number of labels:  16016\n",
      "                                           filepaths       labels\n",
      "0  G:\\Dataset-image-Eye_Data_New\\Keratoconus\\KCN_...  Keratoconus\n",
      "1  G:\\Dataset-image-Eye_Data_New\\Keratoconus\\KCN_...  Keratoconus\n",
      "2  G:\\Dataset-image-Eye_Data_New\\Keratoconus\\KCN_...  Keratoconus\n",
      "3  G:\\Dataset-image-Eye_Data_New\\Keratoconus\\KCN_...  Keratoconus\n",
      "4  G:\\Dataset-image-Eye_Data_New\\Keratoconus\\KCN_...  Keratoconus\n"
     ]
    }
   ],
   "source": [
    "sdir = r'G:\\Dataset-image-Eye_Data_New'\n",
    "\n",
    "slist = os.listdir(sdir)\n",
    "classes = []\n",
    "filepaths = []\n",
    "labels = []\n",
    "\n",
    "for d in slist:\n",
    "    dpath = os.path.join(sdir, d)\n",
    "    if d != \"Test\" and d != \".git\":\n",
    "        if os.path.isdir(dpath):\n",
    "            classes.append(d)\n",
    "\n",
    "class_count = len(classes)\n",
    "\n",
    "for klass in classes:\n",
    "    classpath = os.path.join(sdir, klass)\n",
    "    filelist = os.listdir(classpath)\n",
    "    for f in filelist:\n",
    "        fpath = os.path.join(classpath, f)\n",
    "        filepaths.append(fpath)\n",
    "        labels.append(klass)\n",
    "\n",
    "print('number of files: ', len(filepaths), '   number of labels: ', len(labels))\n",
    "\n",
    "file_series = pd.Series(filepaths, name='filepaths')\n",
    "label_series = pd.Series(labels, name='labels')\n",
    "df = pd.concat([file_series, label_series], axis=1)\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bee25d04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels\n",
      "Normal         5600\n",
      "Keratoconus    5572\n",
      "Suspect        4844\n",
      "Name: count, dtype: int64\n",
      "train size:  12812   test size:  1602    valid size:  1602\n"
     ]
    }
   ],
   "source": [
    "balance = df['labels'].value_counts()\n",
    "print(balance)\n",
    "\n",
    "train_split = .8\n",
    "test_split = .1\n",
    "dummy_split = test_split / (1 - train_split)\n",
    "\n",
    "train_df, dummy_df = train_test_split(df, train_size=train_split, shuffle=True, random_state=125)\n",
    "test_df, valid_df = train_test_split(dummy_df, train_size=dummy_split, shuffle=True, random_state=125)\n",
    "\n",
    "print('train size: ', len(train_df), '  test size: ', len(test_df), '   valid size: ', len(valid_df))\n",
    "length = len(test_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "90eb9fb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 12812 validated image filenames belonging to 3 classes.\n",
      "Found 1602 validated image filenames belonging to 3 classes.\n",
      "Found 1602 validated image filenames belonging to 3 classes.\n"
     ]
    }
   ],
   "source": [
    "batch_size = 16\n",
    "\n",
    "def scalar(x):\n",
    "    return x / 127.5 - 1\n",
    "\n",
    "trgen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "    preprocessing_function=scalar,\n",
    "    horizontal_flip=True\n",
    ")\n",
    "\n",
    "train_gen = trgen.flow_from_dataframe(\n",
    "    train_df,\n",
    "    x_col='filepaths',\n",
    "    y_col='labels',\n",
    "    target_size=(448, 448),\n",
    "    class_mode='categorical',\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    seed=123\n",
    ")\n",
    "\n",
    "tvgen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "    preprocessing_function=scalar\n",
    ")\n",
    "\n",
    "valid_gen = tvgen.flow_from_dataframe(\n",
    "    valid_df,\n",
    "    x_col='filepaths',\n",
    "    y_col='labels',\n",
    "    target_size=(448, 448),\n",
    "    class_mode='categorical',\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "test_batch_size = sorted(\n",
    "    [int(length / n) for n in range(1, length + 1)\n",
    "     if length % n == 0 and length / n <= batch_size],\n",
    "    reverse=True\n",
    ")[0]\n",
    "\n",
    "test_steps = int(length / test_batch_size)\n",
    "\n",
    "test_gen = tvgen.flow_from_dataframe(\n",
    "    test_df,\n",
    "    x_col='filepaths',\n",
    "    y_col='labels',\n",
    "    target_size=(448, 448),\n",
    "    class_mode='categorical',\n",
    "    batch_size=test_batch_size,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "test_labels = test_gen.labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6f09b65b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_training_samples(gen):\n",
    "    class_dict = gen.class_indices\n",
    "    new_dict = {}\n",
    "    for key, value in class_dict.items():\n",
    "        new_dict[value] = key\n",
    "\n",
    "    images, labels = next(gen)\n",
    "    plt.figure(figsize=(15, 15))\n",
    "    length = len(labels)\n",
    "    if length < 25:\n",
    "        r = length\n",
    "    else:\n",
    "        r = 25\n",
    "\n",
    "    for i in range(r):\n",
    "        plt.subplot(5, 5, i + 1)\n",
    "        image = (images[i] + 1) / 2\n",
    "        plt.imshow(image)\n",
    "        index = np.argmax(labels[i])\n",
    "        class_name = new_dict[index]\n",
    "        plt.title(class_name, color='blue', fontsize=16)\n",
    "        plt.axis('off')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bd4d64eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_shape = (448, 448, 3)\n",
    "neurons = 512\n",
    "dropout = .3\n",
    "lr = .001\n",
    "freeze = True\n",
    "\n",
    "base_model = tf.keras.applications.VGG19(include_top=False, input_shape=img_shape, pooling='max', weights='imagenet')\n",
    "\n",
    "if freeze:\n",
    "    base_model.trainable = False\n",
    "\n",
    "x = base_model.output\n",
    "x = tf.keras.layers.BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001)(x)\n",
    "\n",
    "x = tf.keras.layers.Dense(\n",
    "    neurons,\n",
    "    kernel_regularizer=regularizers.l2(0.016),\n",
    "    activity_regularizer=regularizers.l1(0.006),\n",
    "    bias_regularizer=regularizers.l1(0.006),\n",
    "    activation='relu',\n",
    "    kernel_initializer=tf.keras.initializers.GlorotUniform(seed=123)\n",
    ")(x)\n",
    "\n",
    "x = tf.keras.layers.Dropout(rate=dropout, seed=123)(x)\n",
    "\n",
    "output = tf.keras.layers.Dense(\n",
    "    class_count,\n",
    "    activation='softmax',\n",
    "    kernel_initializer=tf.keras.initializers.GlorotUniform(seed=123)\n",
    ")(x)\n",
    "\n",
    "model = Model(inputs=base_model.input, outputs=output)\n",
    "\n",
    "model.compile(Adamax(learning_rate=lr), loss='categorical_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0026bd18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_in_color(txt_msg, fore_tupple, back_tupple):\n",
    "    rf, gf, bf = fore_tupple\n",
    "    rb, gb, bb = back_tupple\n",
    "    msg = '{0}' + txt_msg\n",
    "    mat = '\\33[38;2;' + str(rf) + ';' + str(gf) + ';' + str(bf) + ';48;2;' + str(rb) + ';' + str(gb) + ';' + str(bb) + 'm'\n",
    "    print(msg.format(mat), flush=True)\n",
    "    print('\\33[0m', flush=True)\n",
    "    return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a0d75ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LRA(keras.callbacks.Callback):\n",
    "    def __init__(self, patience, stop_patience, threshold, factor, dwell, model_name, freeze, end_epoch):\n",
    "        super(LRA, self).__init__()\n",
    "        self.patience = patience\n",
    "        self.stop_patience = stop_patience\n",
    "        self.threshold = threshold\n",
    "        self.factor = factor\n",
    "        self.dwell = dwell\n",
    "        self.lr = 0\n",
    "        self.highest_tracc = 0.0\n",
    "        self.lowest_vloss = np.inf\n",
    "        self.count = 0\n",
    "        self.stop_count = 0\n",
    "        self.end_epoch = end_epoch\n",
    "        self.best_weights = None\n",
    "\n",
    "        msg = ' '\n",
    "        if freeze:\n",
    "            msgs = f' Starting training using base model {model_name} with weights frozen...'\n",
    "        else:\n",
    "            msgs = f' Starting training using base model {model_name} training all layers '\n",
    "        print_in_color(msgs, (244, 252, 3), (55, 65, 80))\n",
    "\n",
    "    def set_model(self, model):\n",
    "        self.model_ = model\n",
    "        self.lr = float(tf.keras.backend.get_value(model.optimizer.learning_rate))\n",
    "        self.best_weights = self.model_.get_weights()\n",
    "\n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        if epoch != 0:\n",
    "            msgs = f'for epoch {epoch} '\n",
    "            msgs = msgs + LRA.msg\n",
    "            print_in_color(msgs, (255, 255, 0), (55, 65, 80))\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        lr = float(tf.keras.backend.get_value(self.model_.optimizer.learning_rate))\n",
    "        v_loss = logs.get('val_loss')\n",
    "        acc = logs.get('accuracy')\n",
    "\n",
    "        if acc < self.threshold:\n",
    "            if acc > self.highest_tracc:\n",
    "                LRA.msg = f' training accuracy improved from {self.highest_tracc} to {acc}...'\n",
    "                self.highest_tracc = acc\n",
    "                LRA.best_weights = self.model_.get_weights()\n",
    "                self.count = 0\n",
    "                self.stop_count = 0\n",
    "                if v_loss < self.lowest_vloss:\n",
    "                    self.lowest_vloss = v_loss\n",
    "            else:\n",
    "                if self.count >= self.patience - 1:\n",
    "                    self.lr = lr * self.factor\n",
    "                    self.model_.optimizer.learning_rate.assign(self.lr)\n",
    "                    self.count = 0\n",
    "                    self.stop_count = self.stop_count + 1\n",
    "                    if self.dwell:\n",
    "                        self.model_.set_weights(LRA.best_weights)\n",
    "                else:\n",
    "                    self.count = self.count + 1\n",
    "        else:\n",
    "            if v_loss < self.lowest_vloss:\n",
    "                self.lowest_vloss = v_loss\n",
    "                LRA.best_weights = self.model_.get_weights()\n",
    "                self.count = 0\n",
    "                self.stop_count = 0\n",
    "            else:\n",
    "                if self.count >= self.patience - 1:\n",
    "                    self.lr = self.lr * self.factor\n",
    "                    self.stop_count = self.stop_count + 1\n",
    "                    self.count = 0\n",
    "                    self.model_.optimizer.learning_rate.assign(self.lr)\n",
    "                    if self.dwell:\n",
    "                        self.model_.set_weights(LRA.best_weights)\n",
    "                else:\n",
    "                    self.count = self.count + 1\n",
    "\n",
    "        if epoch == self.end_epoch:\n",
    "            print_in_color(LRA.msg, (255, 255, 0), (55, 65, 80))\n",
    "\n",
    "        if self.stop_count > self.stop_patience - 1:\n",
    "            LRA.msg = f' training halted at epoch {epoch + 1}...'\n",
    "            print_in_color(LRA.msg, (0, 255, 0), (55, 65, 80))\n",
    "            self.model_.stop_training = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b00300e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;2;244;252;3;48;2;55;65;80m Starting training using base model ResNet50 with weights frozen...\n",
      "\u001b[0m\n",
      "Epoch 1/15\n",
      "\u001b[1m  7/801\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:21:19\u001b[0m 6s/step - accuracy: 0.4087 - loss: 27.7365"
     ]
    }
   ],
   "source": [
    "patience=1\n",
    "stop_patience=4\n",
    "threshold=.9\n",
    "factor=.5\n",
    "dwell=False\n",
    "model_type='ResNet50'\n",
    "epochs=15\n",
    "callbacks=[LRA(patience=patience,stop_patience=stop_patience, threshold=threshold, factor=factor,dwell=dwell, model_name=model_type, freeze=freeze, end_epoch=epochs - 1 )]\n",
    "\n",
    "history=model.fit(x=train_gen,  epochs=epochs, verbose=1, callbacks=callbacks,  validation_data=valid_gen, validation_steps=None,  shuffle=False,  initial_epoch=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fec76bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tensorflow.keras.models import load_model\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "model_path = 'G:\\Dataset-image-Eye_Data_New\\Test\\Copy of vgg19_model.h5'\n",
    "print(f\"Loading model from: {model_path}\")\n",
    "model = load_model(model_path)\n",
    "\n",
    "model_type = 'ResNet50'\n",
    "\n",
    "def display_eval_metrics(e_data):\n",
    "    msg='Model Metrics after Training'\n",
    "    print_in_color(msg, (255,255,0), (55,65,80))\n",
    "    msg='{0:^24s}{1:^24s}'.format('Metric', 'Value')\n",
    "    print_in_color(msg, (255,255,0), (55,65,80))\n",
    "    for key,value in e_data.items():\n",
    "        print (f'{key:^24s}{value:^24.5f}')\n",
    "    acc=e_data['accuracy']* 100\n",
    "    return acc\n",
    "\n",
    "subject='Disease 2'\n",
    "save_dir = r'./'\n",
    "\n",
    "print(\"Evaluating model...\")\n",
    "e_dict=model.evaluate(test_gen, batch_size=test_batch_size, verbose=1, steps=test_steps, return_dict=True)\n",
    "acc=display_eval_metrics(e_dict)\n",
    "msg=f'Accuracy on the test set is {acc:5.2f} %'\n",
    "print_in_color(msg, (0,255,0),(55,65,80))\n",
    "\n",
    "print(\"Generating predictions...\")\n",
    "preds=model.predict(test_gen, batch_size=test_batch_size, verbose=0, steps=None)\n",
    "\n",
    "def print_info(test_gen, preds, print_code, save_dir, subject):\n",
    "    class_dict=test_gen.class_indices\n",
    "    labels= test_gen.labels\n",
    "    file_names= test_gen.filenames\n",
    "    error_list=[]\n",
    "    true_class=[]\n",
    "    pred_class=[]\n",
    "    prob_list=[]\n",
    "    new_dict={}\n",
    "    error_indices=[]\n",
    "    y_pred=[]\n",
    "    for key,value in class_dict.items():\n",
    "        new_dict[value]=key\n",
    "\n",
    "    classes=list(new_dict.values())\n",
    "    dict_as_text=str(new_dict)\n",
    "\n",
    "    dict_name= subject + '-' + model_type + '-' + str(len(classes)) +'.txt'\n",
    "    dict_path=os.path.join(save_dir,dict_name)\n",
    "    with open(dict_path, 'w') as x_file:\n",
    "        x_file.write(dict_as_text)\n",
    "    errors=0\n",
    "    for i, p in enumerate(preds):\n",
    "        pred_index=np.argmax(p)\n",
    "        true_index=labels[i]\n",
    "        if pred_index != true_index:\n",
    "            error_list.append(file_names[i])\n",
    "            true_class.append(new_dict[true_index])\n",
    "            pred_class.append(new_dict[pred_index])\n",
    "            prob_list.append(p[pred_index])\n",
    "            error_indices.append(true_index)\n",
    "            errors=errors + 1\n",
    "        y_pred.append(pred_index)\n",
    "    if print_code !=0:\n",
    "        if errors>0:\n",
    "            if print_code>errors:\n",
    "                r=errors\n",
    "            else:\n",
    "                r=print_code\n",
    "            msg='{0:^28s}{1:^28s}{2:^28s}{3:^16s}'.format('Filename', 'Predicted Class' , 'True Class', 'Probability')\n",
    "            print_in_color(msg, (0,255,0),(55,65,80))\n",
    "            for i in range(r):\n",
    "                msg='{0:^28s}{1:^28s}{2:^28s}{3:4s}{4:^6.4f}'.format(error_list[i], pred_class[i],true_class[i], ' ', prob_list[i])\n",
    "                print_in_color(msg, (255,255,255), (55,65,60))\n",
    "        else:\n",
    "            msg='With accuracy of 100 % there are no errors to print'\n",
    "            print_in_color(msg, (0,255,0),(55,65,80))\n",
    "    if errors>0:\n",
    "        plot_bar=[]\n",
    "        plot_class=[]\n",
    "        for key, value in new_dict.items():\n",
    "            count=error_indices.count(key)\n",
    "            if count!=0:\n",
    "                plot_bar.append(count)\n",
    "                plot_class.append(value)\n",
    "        fig=plt.figure()\n",
    "        fig.set_figheight(len(plot_class)/3)\n",
    "        fig.set_figwidth(10)\n",
    "        plt.style.use('fivethirtyeight')\n",
    "        for i in range(0, len(plot_class)):\n",
    "            c=plot_class[i]\n",
    "            x=plot_bar[i]\n",
    "            plt.barh(c, x, )\n",
    "            plt.title(' Errors by Class on Test Set')\n",
    "        plt.savefig(f\"Errors_by_Class_{model_type}.png\", bbox_inches='tight')\n",
    "\n",
    "    if len(classes)<= 20:\n",
    "        y_true= np.array(labels)\n",
    "        y_pred=np.array(y_pred)\n",
    "        cm = confusion_matrix(y_true, y_pred)\n",
    "        clr = classification_report(y_true, y_pred, target_names=classes)\n",
    "        length=len(classes)\n",
    "        if length<8:\n",
    "            fig_width=8\n",
    "            fig_height=8\n",
    "        else:\n",
    "            fig_width=length\n",
    "            fig_height=length\n",
    "        plt.figure(figsize=(fig_width, fig_height))\n",
    "        sns.heatmap(cm, annot=True, vmin=0, fmt='g', cmap='Blues', cbar=False)\n",
    "        plt.xticks(np.arange(length)+.5, classes, rotation= 90)\n",
    "        plt.yticks(np.arange(length)+.5, classes, rotation=0)\n",
    "        plt.xlabel(\"Predicted\")\n",
    "        plt.ylabel(\"Actual\")\n",
    "        plt.title(\"Confusion Matrix\")\n",
    "        plt.savefig(f\"confusion_matrix_{model_type}.png\", bbox_inches='tight')\n",
    "        plt.show()\n",
    "        print(\"Classification Report:\\n----------------------\\n\", clr)\n",
    "\n",
    "print_code=20\n",
    "print_info(test_gen, preds, print_code, save_dir, subject)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "869173a3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
