{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5f3307ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.optimizers import Adam, Adamax\n",
    "from tensorflow.keras.metrics import categorical_crossentropy\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import imshow\n",
    "import seaborn as sns\n",
    "sns.set_style('darkgrid')\n",
    "from sklearn.metrics import confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ffcd0971",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of files:  16016    number of labels:  16016\n",
      "                                           filepaths       labels\n",
      "0  G:\\Dataset-image-Eye_Data_New\\Keratoconus\\KCN_...  Keratoconus\n",
      "1  G:\\Dataset-image-Eye_Data_New\\Keratoconus\\KCN_...  Keratoconus\n",
      "2  G:\\Dataset-image-Eye_Data_New\\Keratoconus\\KCN_...  Keratoconus\n",
      "3  G:\\Dataset-image-Eye_Data_New\\Keratoconus\\KCN_...  Keratoconus\n",
      "4  G:\\Dataset-image-Eye_Data_New\\Keratoconus\\KCN_...  Keratoconus\n"
     ]
    }
   ],
   "source": [
    "sdir = r'G:\\Dataset-image-Eye_Data_New'\n",
    "\n",
    "slist = os.listdir(sdir)\n",
    "classes = []\n",
    "filepaths = []\n",
    "labels = []\n",
    "\n",
    "for d in slist:\n",
    "    dpath = os.path.join(sdir, d)\n",
    "    if d != \"Test\" and d != \".git\":\n",
    "        if os.path.isdir(dpath):\n",
    "            classes.append(d)\n",
    "\n",
    "class_count = len(classes)\n",
    "\n",
    "for klass in classes:\n",
    "    classpath = os.path.join(sdir, klass)\n",
    "    filelist = os.listdir(classpath)\n",
    "    for f in filelist:\n",
    "        fpath = os.path.join(classpath, f)\n",
    "        filepaths.append(fpath)\n",
    "        labels.append(klass)\n",
    "\n",
    "print('number of files: ', len(filepaths), '   number of labels: ', len(labels))\n",
    "\n",
    "file_series = pd.Series(filepaths, name='filepaths')\n",
    "label_series = pd.Series(labels, name='labels')\n",
    "df = pd.concat([file_series, label_series], axis=1)\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bee25d04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels\n",
      "Normal         5600\n",
      "Keratoconus    5572\n",
      "Suspect        4844\n",
      "Name: count, dtype: int64\n",
      "train size:  12812   test size:  1602    valid size:  1602\n"
     ]
    }
   ],
   "source": [
    "balance = df['labels'].value_counts()\n",
    "print(balance)\n",
    "\n",
    "train_split = .8\n",
    "test_split = .1\n",
    "dummy_split = test_split / (1 - train_split)\n",
    "\n",
    "train_df, dummy_df = train_test_split(df, train_size=train_split, shuffle=True, random_state=125)\n",
    "test_df, valid_df = train_test_split(dummy_df, train_size=dummy_split, shuffle=True, random_state=125)\n",
    "\n",
    "print('train size: ', len(train_df), '  test size: ', len(test_df), '   valid size: ', len(valid_df))\n",
    "length = len(test_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "90eb9fb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 12812 validated image filenames belonging to 3 classes.\n",
      "Found 1602 validated image filenames belonging to 3 classes.\n",
      "Found 1602 validated image filenames belonging to 3 classes.\n"
     ]
    }
   ],
   "source": [
    "batch_size = 16\n",
    "\n",
    "def scalar(x):\n",
    "    return x / 127.5 - 1\n",
    "\n",
    "trgen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "    preprocessing_function=scalar,\n",
    "    horizontal_flip=True\n",
    ")\n",
    "\n",
    "train_gen = trgen.flow_from_dataframe(\n",
    "    train_df,\n",
    "    x_col='filepaths',\n",
    "    y_col='labels',\n",
    "    target_size=(448, 448),\n",
    "    class_mode='categorical',\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    seed=123\n",
    ")\n",
    "\n",
    "tvgen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "    preprocessing_function=scalar\n",
    ")\n",
    "\n",
    "valid_gen = tvgen.flow_from_dataframe(\n",
    "    valid_df,\n",
    "    x_col='filepaths',\n",
    "    y_col='labels',\n",
    "    target_size=(448, 448),\n",
    "    class_mode='categorical',\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "test_batch_size = sorted(\n",
    "    [int(length / n) for n in range(1, length + 1)\n",
    "     if length % n == 0 and length / n <= batch_size],\n",
    "    reverse=True\n",
    ")[0]\n",
    "\n",
    "test_steps = int(length / test_batch_size)\n",
    "\n",
    "test_gen = tvgen.flow_from_dataframe(\n",
    "    test_df,\n",
    "    x_col='filepaths',\n",
    "    y_col='labels',\n",
    "    target_size=(448, 448),\n",
    "    class_mode='categorical',\n",
    "    batch_size=test_batch_size,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "test_labels = test_gen.labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6f09b65b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_training_samples(gen):\n",
    "    class_dict = gen.class_indices\n",
    "    new_dict = {}\n",
    "    for key, value in class_dict.items():\n",
    "        new_dict[value] = key\n",
    "\n",
    "    images, labels = next(gen)\n",
    "    plt.figure(figsize=(15, 15))\n",
    "    length = len(labels)\n",
    "    if length < 25:\n",
    "        r = length\n",
    "    else:\n",
    "        r = 25\n",
    "\n",
    "    for i in range(r):\n",
    "        plt.subplot(5, 5, i + 1)\n",
    "        image = (images[i] + 1) / 2\n",
    "        plt.imshow(image)\n",
    "        index = np.argmax(labels[i])\n",
    "        class_name = new_dict[index]\n",
    "        plt.title(class_name, color='blue', fontsize=16)\n",
    "        plt.axis('off')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bd4d64eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_shape = (448, 448, 3)\n",
    "neurons = 512\n",
    "dropout = .3\n",
    "lr = .001\n",
    "freeze = True\n",
    "\n",
    "base_model = tf.keras.applications.VGG19(include_top=False, input_shape=img_shape, pooling='max', weights='imagenet')\n",
    "\n",
    "if freeze:\n",
    "    base_model.trainable = False\n",
    "\n",
    "x = base_model.output\n",
    "x = tf.keras.layers.BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001)(x)\n",
    "\n",
    "x = tf.keras.layers.Dense(\n",
    "    neurons,\n",
    "    kernel_regularizer=regularizers.l2(0.016),\n",
    "    activity_regularizer=regularizers.l1(0.006),\n",
    "    bias_regularizer=regularizers.l1(0.006),\n",
    "    activation='relu',\n",
    "    kernel_initializer=tf.keras.initializers.GlorotUniform(seed=123)\n",
    ")(x)\n",
    "\n",
    "x = tf.keras.layers.Dropout(rate=dropout, seed=123)(x)\n",
    "\n",
    "output = tf.keras.layers.Dense(\n",
    "    class_count,\n",
    "    activation='softmax',\n",
    "    kernel_initializer=tf.keras.initializers.GlorotUniform(seed=123)\n",
    ")(x)\n",
    "\n",
    "model = Model(inputs=base_model.input, outputs=output)\n",
    "\n",
    "model.compile(Adamax(learning_rate=lr), loss='categorical_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0026bd18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_in_color(txt_msg, fore_tupple, back_tupple):\n",
    "    rf, gf, bf = fore_tupple\n",
    "    rb, gb, bb = back_tupple\n",
    "    msg = '{0}' + txt_msg\n",
    "    mat = '\\33[38;2;' + str(rf) + ';' + str(gf) + ';' + str(bf) + ';48;2;' + str(rb) + ';' + str(gb) + ';' + str(bb) + 'm'\n",
    "    print(msg.format(mat), flush=True)\n",
    "    print('\\33[0m', flush=True)\n",
    "    return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a0d75ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LRA(keras.callbacks.Callback):\n",
    "    def __init__(self, patience, stop_patience, threshold, factor, dwell, model_name, freeze, end_epoch):\n",
    "        super(LRA, self).__init__()\n",
    "        self.patience = patience\n",
    "        self.stop_patience = stop_patience\n",
    "        self.threshold = threshold\n",
    "        self.factor = factor\n",
    "        self.dwell = dwell\n",
    "        self.lr = 0\n",
    "        self.highest_tracc = 0.0\n",
    "        self.lowest_vloss = np.inf\n",
    "        self.count = 0\n",
    "        self.stop_count = 0\n",
    "        self.end_epoch = end_epoch\n",
    "        self.best_weights = None\n",
    "\n",
    "        msg = ' '\n",
    "        if freeze:\n",
    "            msgs = f' Starting training using base model {model_name} with weights frozen...'\n",
    "        else:\n",
    "            msgs = f' Starting training using base model {model_name} training all layers '\n",
    "        print_in_color(msgs, (244, 252, 3), (55, 65, 80))\n",
    "\n",
    "    def set_model(self, model):\n",
    "        self.model_ = model\n",
    "        self.lr = float(tf.keras.backend.get_value(model.optimizer.learning_rate))\n",
    "        self.best_weights = self.model_.get_weights()\n",
    "\n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        if epoch != 0:\n",
    "            msgs = f'for epoch {epoch} '\n",
    "            msgs = msgs + LRA.msg\n",
    "            print_in_color(msgs, (255, 255, 0), (55, 65, 80))\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        lr = float(tf.keras.backend.get_value(self.model_.optimizer.learning_rate))\n",
    "        v_loss = logs.get('val_loss')\n",
    "        acc = logs.get('accuracy')\n",
    "\n",
    "        if acc < self.threshold:\n",
    "            if acc > self.highest_tracc:\n",
    "                LRA.msg = f' training accuracy improved from {self.highest_tracc} to {acc}...'\n",
    "                self.highest_tracc = acc\n",
    "                LRA.best_weights = self.model_.get_weights()\n",
    "                self.count = 0\n",
    "                self.stop_count = 0\n",
    "                if v_loss < self.lowest_vloss:\n",
    "                    self.lowest_vloss = v_loss\n",
    "            else:\n",
    "                if self.count >= self.patience - 1:\n",
    "                    self.lr = lr * self.factor\n",
    "                    self.model_.optimizer.learning_rate.assign(self.lr)\n",
    "                    self.count = 0\n",
    "                    self.stop_count = self.stop_count + 1\n",
    "                    if self.dwell:\n",
    "                        self.model_.set_weights(LRA.best_weights)\n",
    "                else:\n",
    "                    self.count = self.count + 1\n",
    "        else:\n",
    "            if v_loss < self.lowest_vloss:\n",
    "                self.lowest_vloss = v_loss\n",
    "                LRA.best_weights = self.model_.get_weights()\n",
    "                self.count = 0\n",
    "                self.stop_count = 0\n",
    "            else:\n",
    "                if self.count >= self.patience - 1:\n",
    "                    self.lr = self.lr * self.factor\n",
    "                    self.stop_count = self.stop_count + 1\n",
    "                    self.count = 0\n",
    "                    self.model_.optimizer.learning_rate.assign(self.lr)\n",
    "                    if self.dwell:\n",
    "                        self.model_.set_weights(LRA.best_weights)\n",
    "                else:\n",
    "                    self.count = self.count + 1\n",
    "\n",
    "        if epoch == self.end_epoch:\n",
    "            print_in_color(LRA.msg, (255, 255, 0), (55, 65, 80))\n",
    "\n",
    "        if self.stop_count > self.stop_patience - 1:\n",
    "            LRA.msg = f' training halted at epoch {epoch + 1}...'\n",
    "            print_in_color(LRA.msg, (0, 255, 0), (55, 65, 80))\n",
    "            self.model_.stop_training = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b00300e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;2;244;252;3;48;2;55;65;80m Starting training using base model ResNet50 with weights frozen...\n",
      "\u001b[0m\n",
      "Epoch 1/5\n",
      "\u001b[1m801/801\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5270s\u001b[0m 7s/step - accuracy: 0.5165 - loss: 6.7364 - val_accuracy: 0.5624 - val_loss: 2.2777\n",
      "\u001b[38;2;255;255;0;48;2;55;65;80mfor epoch 1  training accuracy improved from 0.0 to 0.5164689421653748...\n",
      "\u001b[0m\n",
      "Epoch 2/5\n",
      "\u001b[1m801/801\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5367s\u001b[0m 7s/step - accuracy: 0.5276 - loss: 1.4419 - val_accuracy: 0.6030 - val_loss: 1.0458\n",
      "\u001b[38;2;255;255;0;48;2;55;65;80mfor epoch 2  training accuracy improved from 0.5164689421653748 to 0.5276303291320801...\n",
      "\u001b[0m\n",
      "Epoch 3/5\n",
      "\u001b[1m801/801\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5291s\u001b[0m 7s/step - accuracy: 0.5596 - loss: 1.0113 - val_accuracy: 0.6180 - val_loss: 0.9519\n",
      "\u001b[38;2;255;255;0;48;2;55;65;80mfor epoch 3  training accuracy improved from 0.5276303291320801 to 0.5595535635948181...\n",
      "\u001b[0m\n",
      "Epoch 4/5\n",
      "\u001b[1m801/801\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5398s\u001b[0m 7s/step - accuracy: 0.5717 - loss: 0.9656 - val_accuracy: 0.6330 - val_loss: 0.9125\n",
      "\u001b[38;2;255;255;0;48;2;55;65;80mfor epoch 4  training accuracy improved from 0.5595535635948181 to 0.5716515779495239...\n",
      "\u001b[0m\n",
      "Epoch 5/5\n",
      "\u001b[1m801/801\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6s/step - accuracy: 0.5814 - loss: 0.9421\u001b[38;2;255;255;0;48;2;55;65;80m training accuracy improved from 0.5716515779495239 to 0.581720232963562...\n",
      "\u001b[0m\n",
      "\u001b[1m801/801\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5496s\u001b[0m 7s/step - accuracy: 0.5817 - loss: 0.9362 - val_accuracy: 0.6367 - val_loss: 0.8844\n"
     ]
    }
   ],
   "source": [
    "patience=1\n",
    "stop_patience=4\n",
    "threshold=.9\n",
    "factor=.5\n",
    "dwell=False\n",
    "model_type='ResNet50'\n",
    "epochs=5\n",
    "callbacks=[LRA(patience=patience,stop_patience=stop_patience, threshold=threshold, factor=factor,dwell=dwell, model_name=model_type, freeze=freeze, end_epoch=epochs - 1 )]\n",
    "\n",
    "history=model.fit(x=train_gen,  epochs=epochs, verbose=1, callbacks=callbacks,  validation_data=valid_gen, validation_steps=None,  shuffle=False,  initial_epoch=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d10e4278",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
